{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from joblib import load\n",
    "import pickle\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from radiomics import featureextractor\n",
    "from torchvision.models import resnet\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from monai.networks.nets.resnet import ResNetBottleneck as Bottleneck\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "匹配住院号-影像号-pCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pCR = pd.read_csv(r\"C:\\Users\\JackonHong\\Desktop\\PyRadiomics_feature\\samples_info_2021.csv\")\n",
    "# ID = pd.read_csv(r\"C:\\\\Users\\\\JackonHong\\\\Desktop\\\\PyRadiomics_feature\\\\Patient_ID_Match.csv\",encoding=\"GB2312\")\n",
    "# merged_df = pd.merge(pCR, ID, how='inner', left_on='ID', right_on='住院号')\n",
    "\n",
    "# k = pd.DataFrame(np.concatenate((merged_df.values[:,29].reshape(-1,1), merged_df.values[:,9:13].reshape(-1,4)),axis=1))\n",
    "# # k.to_csv(\"valid_label_type.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_label = pd.read_csv(r\"./Valid_label.csv\")\n",
    "# k = pd.read_csv(r\"./valid_label_type.csv\")\n",
    "# merged_df = pd.merge(valid_label, k, how='inner', on='ID')\n",
    "# merged_df.to_csv(\"kkk.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pCR->1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor =  pd.read_csv(\"./pyra_tumor_[111].csv\").values\n",
    "border = pd.read_csv(\"./pyra_border9_[111].csv\").values\n",
    "DL = pd.read_csv(\"./DL.csv\").values\n",
    "valid_tumor =  pd.read_csv(\"./valid_tumor_[111].csv\").values\n",
    "valid_border = pd.read_csv(\"./valid_border9_[111].csv\").values\n",
    "valid_DL = pd.read_csv(\"./valid_DL.csv\").values\n",
    "\n",
    "label = pd.read_csv(\"./Train_label.csv\").values[:,5]\n",
    "label = np.array(label, dtype=float)\n",
    "\n",
    "valid_label = pd.read_csv(\"./Valid_label.csv\")\n",
    "pcr_values = valid_label.values[:,1]\n",
    "pcr_values[pcr_values <= 0] = 0\n",
    "pcr_values[pcr_values > 0] = 1\n",
    "valid_label.iloc[:,1] = pcr_values\n",
    "\n",
    "\n",
    "ROI_path = \"H:\\\\new\\\\ROI\"\n",
    "ROI_list = os.listdir(ROI_path)\n",
    "ROI_list.sort()\n",
    "ROI_list = np.array(ROI_list).astype(int)\n",
    "ROI_dataFrame = pd.DataFrame(ROI_list,dtype=object)\n",
    "ROI_dataFrame.columns = ['name']\n",
    "\n",
    "\n",
    "valid_label = pd.merge(ROI_dataFrame, valid_label, how='inner', left_on=\"name\", right_on=\"ID\")\n",
    "\n",
    "valid_label = valid_label.values[:,2]\n",
    "valid_label = np.array(valid_label, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "tumor = min_max_scaler.fit_transform(standard_scaler.fit_transform(tumor))\n",
    "valid_tumor = min_max_scaler.fit_transform(standard_scaler.fit_transform(valid_tumor))\n",
    "border = min_max_scaler.fit_transform(standard_scaler.fit_transform(border))\n",
    "valid_border = min_max_scaler.fit_transform(standard_scaler.fit_transform(valid_border))\n",
    "DL = min_max_scaler.fit_transform(standard_scaler.fit_transform(DL))\n",
    "valid_DL = min_max_scaler.fit_transform(standard_scaler.fit_transform(valid_DL))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并数据\n",
    "tumor_border = np.concatenate((tumor,border),axis=1)\n",
    "DL_tumor_border = np.concatenate((DL, tumor_border),axis=1)\n",
    "\n",
    "valid_tumor_border = np.concatenate((valid_tumor,valid_border),axis=1)\n",
    "valid_DL_tumor_border = np.concatenate((valid_DL, valid_tumor_border),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_learning(tumor_select, label, valid_tumor_select, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        LogisticRegression(),\n",
    "        SVC(probability=True, random_state=42),\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        KNeighborsClassifier(),\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        MLPClassifier(max_iter=200, random_state=42),\n",
    "        AdaBoostClassifier(random_state=42),\n",
    "        ExtraTreesClassifier(),\n",
    "    ]\n",
    "i = 0\n",
    "for model in models:\n",
    "    print(f\"{model.__class__.__name__}\")\n",
    "    tumor_select = tumor_select_all[i]\n",
    "    valid_tumor_select = valid_tumor_select_all[i]\n",
    "    print(Machine_Learning_alpha(tumor_select, label, valid_tumor_select, valid_label, model))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记录保存选择特征 勾画曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        LogisticRegression(),\n",
    "        SVC(probability=True, random_state=42),\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        KNeighborsClassifier(),\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        MLPClassifier(max_iter=200, random_state=42),\n",
    "        AdaBoostClassifier(random_state=42),\n",
    "        ExtraTreesClassifier(),\n",
    "    ]\n",
    "best_auc_all = []\n",
    "tumor_select_all = []\n",
    "valid_tumor_select_all = []\n",
    "index_all = []\n",
    "for model in models:\n",
    "    best_auc, tumor_select, valid_tumor_select, index = feature_select_auc(tumor, label, valid_tumor, valid_label,model)\n",
    "    best_auc_all.append(best_auc)\n",
    "    tumor_select_all.append(tumor_select)\n",
    "    valid_tumor_select_all.append(valid_tumor_select)\n",
    "    index_all.append(index)\n",
    "    with open('tumor_select.pkl', 'wb') as file:\n",
    "        pickle.dump(tumor_select_all, file) \n",
    "    with open('valid_tumor_select.pkl', 'wb') as file:\n",
    "        pickle.dump(valid_tumor_select_all, file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tumor_select.pkl\n",
    "valid_tumor_select.pkl\n",
    "\n",
    "border_select.pkl\n",
    "valid_border_select.pkl\n",
    "\n",
    "DL_select.pkl\n",
    "valid_DL_select.pkl\n",
    "\n",
    "tumor_border_select.pkl\n",
    "valid_tumor_border_select.pkl\n",
    "\n",
    "DL_tumor_border_select.pkl\n",
    "valid_DL_tumor_border_select.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from operator import itemgetter\n",
    "\n",
    "# 打开之前序列化的对象的文件\n",
    "with open('tumor_select.pkl', 'rb') as file:\n",
    "    # 使用pickle加载序列化的对象\n",
    "    tumor_select_1 = pickle.load(file)\n",
    "with open('valid_tumor_select.pkl', 'rb') as file:\n",
    "    # 使用pickle加载序列化的对象\n",
    "    valid_tumor_select_1 = pickle.load(file)\n",
    "\n",
    "optimized_models = {\n",
    "        \"LR\": LogisticRegression(),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"KNeighbors\": KNeighborsClassifier(),\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "        \"GradientBoost\": GradientBoostingClassifier(random_state=42),\n",
    "        \"MLP\": MLPClassifier(max_iter=200, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    }\n",
    "i = 0\n",
    "results_test_dict = {}\n",
    "results_valid_dict = {}\n",
    "kflod_test_index = []\n",
    "kflod_valid_index = []\n",
    "\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    print(name)\n",
    "\n",
    "    tumor_select = tumor_select_1[i]\n",
    "    valid_tumor_select = valid_tumor_select_1[i]\n",
    "\n",
    "    results_table = Machine_Learning_alpha(tumor_select, label, valid_tumor_select, valid_label, model)\n",
    "    test_result=[]\n",
    "    valid_result=[]\n",
    "    for j in range(0,10,2):\n",
    "        test_result.append(results_table[j])\n",
    "        valid_result.append(results_table[j+1])\n",
    "\n",
    "    #排序AUC结果\n",
    "    enumerated_metrics = list(enumerate(test_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "    max_auc_element = sorted_metrics[0]\n",
    "    k = max_auc_element[1]  \n",
    "    max_index = max_auc_element[0]  \n",
    "    kflod_test_index.append(max_index)\n",
    "    results_test_dict[name] = k\n",
    "\n",
    "    enumerated_metrics = list(enumerate(valid_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "\n",
    "    if k['AUC'][0] > sorted_metrics[1][1]['AUC'][0] and (k['AUC'][0]-sorted_metrics[1][1]['AUC'][0]) < 0.1:\n",
    "        results_valid_dict[name] = sorted_metrics[1][1]\n",
    "        max_auc_element = sorted_metrics[1]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n",
    "    else:\n",
    "        results_valid_dict[name] = sorted_metrics[0][1]\n",
    "        max_auc_element = sorted_metrics[0]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n",
    "\n",
    "    # max_auc_element = sorted_metrics[0]\n",
    "    # max_index = max_auc_element[0]\n",
    "    # kflod_valid_index.append(max_index)\n",
    "\n",
    "    # sorted_metrics = sorted(valid_result, key=lambda x: x['AUC'][0], reverse=True)\n",
    "    # print(sorted_metrics)\n",
    "    # if k['AUC'][0] > sorted_metrics[1]['AUC'][0] and (k['AUC'][0]-sorted_metrics[1]['AUC'][0]) < 0.1:\n",
    "    #     results_valid_dict[name] = sorted_metrics[1]\n",
    "    # else:\n",
    "    #     results_valid_dict[name] = sorted_metrics[0]\n",
    "    # 1/0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimized_models = {\n",
    "        \"LR\": LogisticRegression(),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"KNeighbors\": KNeighborsClassifier(),\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "        \"GradientBoost\": GradientBoostingClassifier(random_state=42),\n",
    "        \"MLP\": MLPClassifier(max_iter=200, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    }\n",
    "i = 0\n",
    "\n",
    "results_test_dict = {}\n",
    "results_valid_dict = {}\n",
    "kflod_test_index = []\n",
    "kflod_valid_index = []\n",
    "\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    print(name)\n",
    "\n",
    "    tumor_select = tumor_select_1[i]\n",
    "    valid_tumor_select = valid_tumor_select_1[i]\n",
    "\n",
    "    results_table = Machine_Learning_alpha(tumor_select, label, valid_tumor_select, valid_label, model)\n",
    "    test_result=[]\n",
    "    valid_result=[]\n",
    "    for j in range(0,10,2):\n",
    "        test_result.append(results_table[j])\n",
    "        valid_result.append(results_table[j+1])\n",
    "\n",
    "    #排序AUC结果\n",
    "    enumerated_metrics = list(enumerate(test_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "    max_auc_element = sorted_metrics[0]\n",
    "    k = max_auc_element[1]  \n",
    "    max_index = max_auc_element[0]  \n",
    "    kflod_test_index.append(max_index)\n",
    "    results_test_dict[name] = k\n",
    "\n",
    "    enumerated_metrics = list(enumerate(valid_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "\n",
    "    if k['AUC'][0] > sorted_metrics[1][1]['AUC'][0] and (k['AUC'][0]-sorted_metrics[1][1]['AUC'][0]) < 0.1:\n",
    "        results_valid_dict[name] = sorted_metrics[1][1]\n",
    "        max_auc_element = sorted_metrics[1]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n",
    "    else:\n",
    "        results_valid_dict[name] = sorted_metrics[0][1]\n",
    "        max_auc_element = sorted_metrics[0]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n",
    "\n",
    "    # max_auc_element = sorted_metrics[0]\n",
    "    # max_index = max_auc_element[0]\n",
    "    # kflod_valid_index.append(max_index)\n",
    "\n",
    "    # sorted_metrics = sorted(valid_result, key=lambda x: x['AUC'][0], reverse=True)\n",
    "    # print(sorted_metrics)\n",
    "    # if k['AUC'][0] > sorted_metrics[1]['AUC'][0] and (k['AUC'][0]-sorted_metrics[1]['AUC'][0]) < 0.1:\n",
    "    #     results_valid_dict[name] = sorted_metrics[1]\n",
    "    # else:\n",
    "    #     results_valid_dict[name] = sorted_metrics[0]\n",
    "    # 1/0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_models = {\n",
    "        \"LR\": LogisticRegression(),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"KNeighbors\": KNeighborsClassifier(),\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "        \"GradientBoost\": GradientBoostingClassifier(random_state=42),\n",
    "        \"MLP\": MLPClassifier(max_iter=200, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    }\n",
    "plot_curves(optimized_models, tumor_select_1, label, valid_tumor_select_1, valid_label, results_test_dict, results_valid_dict, \n",
    "            kflod_test_index, kflod_valid_index, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各组学在单一模型下的对比效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 97)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tumor\\border\\tumor_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_values = select_data['select_3'][0]  # 929*58 \n",
    "tumor_values = tumor_border  # 929*1609\n",
    " \n",
    "\n",
    "tumor_1 =  pd.read_csv(\"./pyra_tumor_[111].csv\")\n",
    "border_1 = pd.read_csv(\"./pyra_border9_[111].csv\")\n",
    "tumor_border_1 = pd.concat([tumor_1,border_1], axis=1)\n",
    "# 初始化一个列表，用于存储匹配的列索引\n",
    "indices = []\n",
    "\n",
    "# 遍历 select_values 的每一列\n",
    "for i in range(select_values.shape[1]):\n",
    "    # 获取 select_values 的第 i 列\n",
    "    select_column = select_values[:, i]\n",
    "    \n",
    "    # 在 tumor_values 中查找完全匹配的列\n",
    "    for j in range(tumor_values.shape[1]):\n",
    "        if np.array_equal(select_column, tumor_values[:, j]):\n",
    "            indices.append(j)\n",
    "            break  # 找到匹配列后跳出内层循环\n",
    "\n",
    "# 将索引列表转换为 NumPy 数组\n",
    "indices = sorted(np.array(indices))\n",
    "new_table = tumor_border_1.iloc[:,indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_select_files[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     26\u001b[0m         valid_select_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_select_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 假设你需要对每个select_data中的元素应用模型，这里演示如何访问\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "optimized_models = LogisticRegression()\n",
    "\n",
    "select_files = ['tumor_select', 'border_select', 'DL_select', 'tumor_border_select', 'DL_tumor_border_select']\n",
    "valid_select_files = ['valid_tumor_select', 'valid_border_select', 'valid_DL_select', 'valid_tumor_border_select', 'valid_DL_tumor_border_select']\n",
    "#select_files = ['DL_tumor_border_select']\n",
    "#valid_select_files = ['valid_DL_tumor_border_select']\n",
    "\n",
    "# 加载数据\n",
    "select_data = {}\n",
    "valid_select_data = {}\n",
    "\n",
    "results_test_dict = {}\n",
    "results_valid_dict = {}\n",
    "kflod_test_index = []\n",
    "kflod_valid_index = []\n",
    "\n",
    "for i in range(5):\n",
    "    with open(f'{select_files[i]}.pkl', 'rb') as file:\n",
    "        select_data[f'select_{i}'] = pickle.load(file)\n",
    "    with open(f'{valid_select_files[i]}.pkl', 'rb') as file:\n",
    "        valid_select_data[f'valid_select_{i}'] = pickle.load(file)\n",
    "1/0\n",
    "for i in range(5):\n",
    "    tumor_select = select_data[f'select_{i}'][0]  # 动态获取变量\n",
    "    valid_tumor_select = valid_select_data[f'valid_select_{i}'][0]  # 动态获取验证集变量\n",
    "    \n",
    "    results_table = Machine_Learning_alpha(tumor_select, label, valid_tumor_select, valid_label, optimized_models)\n",
    "    test_result=[]\n",
    "    valid_result=[]\n",
    "    for j in range(0,10,2):\n",
    "        test_result.append(results_table[j])\n",
    "        valid_result.append(results_table[j+1])\n",
    "\n",
    "    enumerated_metrics = list(enumerate(test_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "    max_auc_element = sorted_metrics[0]\n",
    "    k = max_auc_element[1]  \n",
    "    max_index = max_auc_element[0]  \n",
    "    kflod_test_index.append(max_index)\n",
    "    results_test_dict[select_files[i]] = k\n",
    "\n",
    "    enumerated_metrics = list(enumerate(valid_result))\n",
    "    sorted_metrics = sorted(enumerated_metrics, key=lambda x: x[1]['AUC'][0], reverse=True)\n",
    "\n",
    "    if k['AUC'][0] > sorted_metrics[1][1]['AUC'][0] and (k['AUC'][0]-sorted_metrics[1][1]['AUC'][0]) < 0.1:\n",
    "        results_valid_dict[valid_select_files[i]] = sorted_metrics[1][1]\n",
    "        max_auc_element = sorted_metrics[1]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n",
    "    else:\n",
    "        results_valid_dict[valid_select_files[i]] = sorted_metrics[0][1]\n",
    "        max_auc_element = sorted_metrics[0]\n",
    "        max_index = max_auc_element[0]\n",
    "        kflod_valid_index.append(max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_model_plot_curves(optimized_models, select_data, label, valid_select_data, valid_label, results_test_dict, results_valid_dict, \n",
    "            kflod_test_index, kflod_valid_index, False, select_files, valid_select_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_border_select = np.concatenate((tumor_select,border_select),axis=1)\n",
    "DL_tumor_border_select = np.concatenate((DL_select, tumor_border_select),axis=1)\n",
    "\n",
    "valid_tumor_border_select = np.concatenate((valid_tumor_select,valid_border_select),axis=1)\n",
    "valid_DL_tumor_border_select = np.concatenate((valid_DL_select, valid_tumor_border_select),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_1, tumor_select, valid_tumor_select, index_1 = feature_select_auc(tumor, label, valid_tumor, valid_label)\n",
    "best_auc_2, border_select, valid_border_select, index_2 = feature_select_auc(border, label, valid_border, valid_label)\n",
    "best_auc_3, DL_select, valid_DL_select, index_3 = feature_select_auc(DL, label, valid_DL, valid_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyradiomics特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_path = r\"H:\\new\\ROI\"\n",
    "image_list = os.listdir(valid_image_path)\n",
    "image_list.sort()\n",
    "\n",
    "settings = {}\n",
    "settings['binWidth'] = 25\n",
    "settings['sigma'] = [3, 5]\n",
    "settings['normalize'] = False\n",
    "settings['resampledPixelSpacing'] = None\n",
    "settings['interpolator'] = sitk.sitkBSpline\n",
    "\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(**settings)\n",
    "extractor.enableAllImageTypes()\n",
    "extractor.enableAllFeatures()\n",
    "feature_total_all = []\n",
    "feature_border_all = []\n",
    "try:\n",
    "    for i in range(len(image_list)):\n",
    "        ID = int(image_list[i])\n",
    "        print(ID)\n",
    "        print(i)\n",
    "        image_path, mask_path = path_find(valid_image_path, image_list, i)\n",
    "\n",
    "        image = sitk.ReadImage(image_path)\n",
    "        mask = sitk.ReadImage(mask_path)\n",
    "        image_array = sitk.GetArrayFromImage(image)\n",
    "        mask_array = sitk.GetArrayFromImage(mask)\n",
    "        #最大切片裁剪\n",
    "        image_array = image_array[0,:,:,:]\n",
    "        mask_array = mask_array[0,:,:,:]\n",
    "        \n",
    "        image = sitk.GetImageFromArray(image_array)\n",
    "        mask = sitk.GetImageFromArray(mask_array)\n",
    "        image = resample_img(image)\n",
    "        image = sitk.Normalize(image)\n",
    "        mask = resample_img(mask,is_label=True)\n",
    "\n",
    "        image_array = sitk.GetArrayFromImage(image)\n",
    "        mask_array = sitk.GetArrayFromImage(mask)\n",
    "        iteration = 9 # newSpacing是1mm,扩展3mm\n",
    "        mask_img_arr_expand = expand_dilation(mask_array,iterations=iteration)\n",
    "        mask_img_arr_border = diffROI(mask_array,mask_img_arr_expand)\n",
    "        mask_img_arr_border = sitk.GetImageFromArray(mask_img_arr_border)\n",
    "        img = sitk.GetImageFromArray(image_array)\n",
    "        feature_border = extractor.execute(img, mask_img_arr_border, label = 1)\n",
    "\n",
    "        feature_border_all.append(feature_border)\n",
    "except Exception as e:\n",
    "    print(ID)\n",
    "\n",
    "result = pd.DataFrame(feature_border_all)\n",
    "csv_filename_1 = \"valid_border9_[111].csv\"\n",
    "result.to_csv(csv_filename_1, index=False)\n",
    "    # area = np.sum(mask_array == 1, axis=(1, 2))\n",
    "    # area_index = np.argsort(area)[-1]\n",
    "    # img = image_array[area_index, :, :]\n",
    "    # mask = mask_array[area_index, :, :]\n",
    "    # 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_path = r\"H:\\new\\ROI\"\n",
    "image_list = os.listdir(valid_image_path)\n",
    "image_list.sort()\n",
    "\n",
    "settings = {}\n",
    "settings['binWidth'] = 25\n",
    "settings['sigma'] = [3, 5]\n",
    "settings['normalize'] = True\n",
    "settings['resampledPixelSpacing'] = [1,1,1]\n",
    "settings['interpolator'] = sitk.sitkBSpline\n",
    "\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(**settings)\n",
    "extractor.enableAllImageTypes()\n",
    "extractor.enableAllFeatures()\n",
    "feature_total_all = []\n",
    "feature_border_all = []\n",
    "try:\n",
    "    for i in range(len(image_list)):\n",
    "        ID = int(image_list[i])\n",
    "        print(ID)\n",
    "        print(i)\n",
    "        image_path, mask_path = path_find(valid_image_path, image_list, i)\n",
    "\n",
    "        image = sitk.ReadImage(image_path)\n",
    "        mask = sitk.ReadImage(mask_path)\n",
    "        image_array = sitk.GetArrayFromImage(image)\n",
    "        mask_array = sitk.GetArrayFromImage(mask)\n",
    "        #最大切片裁剪\n",
    "        image_array = image_array[0,:,:,:]\n",
    "        mask_array = mask_array[0,:,:,:]\n",
    "        \n",
    "        image = sitk.GetImageFromArray(image_array)\n",
    "        mask = sitk.GetImageFromArray(mask_array)\n",
    "        # image = resample_img(image)\n",
    "        # image = sitk.Normalize(image)\n",
    "        # mask = resample_img(mask,is_label=True)\n",
    "\n",
    "        # image_array = sitk.GetArrayFromImage(image)\n",
    "        # mask_array = sitk.GetArrayFromImage(mask)\n",
    "        # iteration = 1 # newSpacing是1mm,扩展3mm\n",
    "        # mask_img_arr_expand = expand_dilation(mask_array,iterations=iteration)\n",
    "        # mask_img_arr_border = diffROI(mask_array,mask_img_arr_expand)\n",
    "        # mask_img_arr_border = sitk.GetImageFromArray(mask_img_arr_border)\n",
    "        # img = sitk.GetImageFromArray(image_array)\n",
    "        feature_border = extractor.execute(image, mask, label = 1)\n",
    "\n",
    "        feature_border_all.append(feature_border)\n",
    "except Exception as e:\n",
    "    print(ID)\n",
    "\n",
    "result = pd.DataFrame(feature_border_all)\n",
    "csv_filename_1 = \"valid_tumor_[111].csv\"\n",
    "result.to_csv(csv_filename_1, index=False)\n",
    "    # area = np.sum(mask_array == 1, axis=(1, 2))\n",
    "    # area_index = np.argsort(area)[-1]\n",
    "    # img = image_array[area_index, :, :]\n",
    "    # mask = mask_array[area_index, :, :]\n",
    "    # 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_find(valid_image_path, image_list, i):\n",
    "    image_path = valid_image_path + \"\\\\\" + image_list[i]\n",
    "    date_list = os.listdir(image_path)\n",
    "    date_objects = [datetime.strptime(date, '%Y-%m-%d') for date in date_list]\n",
    "    earliest_date = min(date_objects)\n",
    "    index = date_objects.index(earliest_date)   \n",
    "    image_path = image_path + \"\\\\\" + os.listdir(image_path)[index]\n",
    "    files = os.listdir(image_path)\n",
    "    target_image = [file for file in files if \"image\" in file ]\n",
    "    target_ROI = [file for file in files if \"New\" in file ]\n",
    "\n",
    "    target_image_paths = [os.path.join(image_path, file) for file in target_image]\n",
    "    target_mask_paths = [os.path.join(image_path, file) for file in target_ROI]\n",
    "    \n",
    "    return target_image_paths, target_mask_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_img(itk_image, out_spacing=[1.0, 1.0, 1.0], is_label=False):\n",
    "    # resample images to 2mm spacing with simple itk\n",
    "\n",
    "    original_spacing = itk_image.GetSpacing()\n",
    "    original_size = itk_image.GetSize()\n",
    "\n",
    "    out_size = [\n",
    "        int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n",
    "        int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n",
    "        int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))]\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size)\n",
    "    resample.SetOutputDirection(itk_image.GetDirection())\n",
    "    resample.SetOutputOrigin(itk_image.GetOrigin())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(itk_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dilation(mask_img_arr,iterations = 5):\n",
    "    shape_nrrd = mask_img_arr.shape\n",
    "    mask_img_arr_expand = np.zeros(shape_nrrd)\n",
    "    for index in range(shape_nrrd[0]):\n",
    "        mask_img_arr_expand[index,:,:] = scipy.ndimage.binary_dilation(mask_img_arr[index,:,:],iterations = iterations).astype('uint16')\n",
    "    return mask_img_arr_expand\n",
    "\n",
    "\n",
    "def diffROI(mask_img_arr,mask_img_arr_expand):\n",
    "# 通过传入原始ROI mask_img_arr和扩展了5mm的ROI mask_img_arr_expand\n",
    "# 返回扩展的5mm的ROI区域\n",
    "    # 首先把二值ROI都转为0和1 \n",
    "    mask_img_arr[mask_img_arr != 0] = 1\n",
    "    mask_img_arr_expand[mask_img_arr_expand != 0] = 1\n",
    "\n",
    "    shape_nrrd = mask_img_arr.shape\n",
    "    mask_img_arr_border = np.zeros(shape_nrrd)\n",
    "    for index in range(shape_nrrd[0]):\n",
    "        for x in range(shape_nrrd[1]):\n",
    "            for y in range(shape_nrrd[2]):\n",
    "                if mask_img_arr[index,x,y] != mask_img_arr_expand[index,x,y]:\n",
    "                    mask_img_arr_border[index,x,y] = 1\n",
    "    return mask_img_arr_border\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_path = r\"H:\\new\\ROI\"\n",
    "image_list = os.listdir(valid_image_path)\n",
    "image_list.sort()\n",
    "\n",
    "# try:\n",
    "for i in range(len(image_list)):\n",
    "    ID = int(image_list[i])\n",
    "    print(ID)\n",
    "    print(i)\n",
    "    image_path, mask_path = path_find(valid_image_path, image_list, i)\n",
    "\n",
    "    image = sitk.ReadImage(image_path)\n",
    "    mask = sitk.ReadImage(mask_path)\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "    #最大切片裁剪\n",
    "    image_array = image_array[0,:,:,:]\n",
    "    mask_array = mask_array[0,:,:,:]\n",
    "    \n",
    "    image = sitk.GetImageFromArray(image_array)\n",
    "    mask = sitk.GetImageFromArray(mask_array)\n",
    "    image = resample_img(image)\n",
    "    image = sitk.Normalize(image)\n",
    "    mask = resample_img(mask,is_label=True)\n",
    "\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "    area = np.sum(mask_array == 1, axis=(1, 2))\n",
    "    area_index = np.argsort(area)[-1]\n",
    "    img = image_array[area_index, :, :]\n",
    "    mask = mask_array[area_index, :, :]\n",
    "    mask = expand_mask(mask)\n",
    "    1/0\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(ID)\n",
    "\n",
    "result = pd.DataFrame(feature_border_all)\n",
    "csv_filename_1 = \"valid_border1_pyra_slice_resample[555]_normal.csv\"\n",
    "result.to_csv(csv_filename_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\JackonHong\\Desktop\\1.resample[111]2.normal3.slice\\image\"\n",
    "mask_path = r\"C:\\Users\\JackonHong\\Desktop\\1.resample[111]2.normal3.slice\\mask\"\n",
    "image_list = os.listdir(image_path)\n",
    "image_list.sort()\n",
    "mask_list = os.listdir(mask_path)\n",
    "mask_list.sort()\n",
    "ID_total = []\n",
    "image_max_slic_total = []\n",
    "for i in range(len(mask_list)):\n",
    "    ID = image_list[i][0:6]\n",
    "    print(ID)\n",
    "    image = sitk.ReadImage(image_path + \"\\\\\" + image_list[i])\n",
    "    mask = sitk.ReadImage(mask_path + \"\\\\\" + mask_list[i])\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "    #slic_max\n",
    "    area = np.sum(mask_array == 0, axis=(1, 2))\n",
    "    area_index = np.argsort(area)[-1]\n",
    "    img = image_array[area_index, :, :]\n",
    "    mask = mask_array[area_index, :, :]\n",
    "    mask[mask>0] = 1\n",
    "    mask[mask == 1] = 10\n",
    "    mask[mask == 0] = 1 \n",
    "    mask[mask == 10] = 0\n",
    "    mask = expand_mask(mask)\n",
    "    a = np.where(mask == 1)\n",
    "    image = img[a[0].min():a[0].max(),a[1].min():a[1].max()]\n",
    "    output_shape = [32,32]\n",
    "    sample = scipy.ndimage.zoom(image, zoom=(output_shape[0]/image.shape[0],output_shape[1]/image.shape[1]), output=None, order=3, mode='constant', cval=0.0, prefilter=True)\n",
    "    image_max_slic_total.append(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_path = r\"H:\\new\\ROI\"\n",
    "image_list = os.listdir(valid_image_path)\n",
    "image_list.sort()\n",
    "image_max_slic_total = []\n",
    "# try:\n",
    "for i in range(len(image_list)):\n",
    "    ID = int(image_list[i])\n",
    "    print(ID)\n",
    "    print(i)\n",
    "    image_path, mask_path = path_find(valid_image_path, image_list, i)\n",
    "\n",
    "    image = sitk.ReadImage(image_path)\n",
    "    mask = sitk.ReadImage(mask_path)\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "    \n",
    "    image_array = image_array[0,:,:,:]\n",
    "    mask_array = mask_array[0,:,:,:]\n",
    "    \n",
    "    image = sitk.GetImageFromArray(image_array)\n",
    "    mask = sitk.GetImageFromArray(mask_array)\n",
    "    image = resample_img(image)\n",
    "    image = sitk.Normalize(image)\n",
    "    mask = resample_img(mask,is_label=True)\n",
    "\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "    #最大切片裁剪\n",
    "    area = np.sum(mask_array == 1, axis=(1, 2))\n",
    "    area_index = np.argsort(area)[-1]\n",
    "    img = image_array[area_index, :, :]\n",
    "    mask = mask_array[area_index, :, :]\n",
    "    mask = expand_mask(mask)\n",
    "    a = np.where(mask == 1)\n",
    "    image = img[a[0].min():a[0].max(),a[1].min():a[1].max()]\n",
    "    1/0\n",
    "    output_shape = [32,32]\n",
    "    sample = scipy.ndimage.zoom(image, zoom=(output_shape[0]/image.shape[0],output_shape[1]/image.shape[1]), output=None, order=3, mode='constant', cval=0.0, prefilter=True)\n",
    "    image_max_slic_total.append(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mask(mask):\n",
    "    # 定义膨胀核的大小（以像素为单位）\n",
    "    kernel_size = 9\n",
    "    # 构造膨胀核\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    # 对肿瘤区域进行膨胀操作\n",
    "    dilated_mask = cv2.dilate(mask, kernel)\n",
    "    # 找到膨胀后肿瘤区域的边界框\n",
    "    nonzero_indices = np.argwhere(dilated_mask)\n",
    "    ymin, xmin = np.min(nonzero_indices, axis=0)\n",
    "    ymax, xmax = np.max(nonzero_indices, axis=0)\n",
    "\n",
    "    # 构建长方形区域\n",
    "    rectangular_region = np.zeros_like(dilated_mask)\n",
    "    rectangular_region[ymin:ymax+1, xmin:xmax+1] = 1\n",
    "\n",
    "    return rectangular_region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取深度学习特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "MRI_array_datasets = torch.Tensor(np.array(image_max_slic_total)[:, np.newaxis, :, :])\n",
    "MRI_array_datasets = MRI_array_datasets.expand(-1, 3, -1, -1)\n",
    "\n",
    "# 定义标准化变换\n",
    "normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# 应用标准化变换\n",
    "MRI_array_datasets = normalize_transform(MRI_array_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "data = MRI_array_datasets\n",
    "# 确保数据是浮点类型\n",
    "data = data.float()\n",
    "\n",
    "# data = data.repeat(1, 3, 1, 1)  # 重复通道维度以匹配3通道的输入\n",
    "# data = torch.nn.functional.interpolate(data, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "resnet50.eval()\n",
    "children = list(resnet50.children())\n",
    "\n",
    "del children[-1]\n",
    "\n",
    "resnet50 = torch.nn.Sequential(*children)\n",
    "with torch.no_grad():\n",
    "    # 获取特征\n",
    "    features = resnet50(data)\n",
    "\n",
    "features = features.squeeze(2)  # 移除单维度的高度维度\n",
    "print(\"Feature shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features.reshape(929,2048).cpu()).to_csv(\"DL.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#肿瘤、瘤周、DL特征读取\n",
    "\n",
    "external_valid_tumor_feature = pd.read_csv(\"./valid_pyra_slice_resample[555]_normal.csv\")\n",
    "external_valid_border_feature = pd.read_csv(\"./valid_border1_pyra_slice_resample[555]_normal.csv\")\n",
    "external_valid_DL_feature = pd.read_csv(\"./external_valid_resample[555]_feature_resnet50_NoPretrain_3.csv\")\n",
    "\n",
    "# label = external_valid_DL_feature.values[:,1]\n",
    "\n",
    "# external_valid_tumor_feature = external_valid_tumor_feature.values[:,1:]\n",
    "# external_valid_border_feature = external_valid_border_feature.values[:,1:]\n",
    "# external_valid_DL_feature = external_valid_DL_feature.values[:,2:]\n",
    "\n",
    "#初次筛选三类组学的下标读取\n",
    "tumor_select = pd.read_csv(\"./tumor_select.csv\")\n",
    "border_select = pd.read_csv(\"./border_select.csv\")\n",
    "DL_select = pd.read_csv(\"./DL_select.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_valid_tumor_select_feature = []\n",
    "for i in range(len(tumor_select.columns[1:])):\n",
    "    k = int(tumor_select.columns[1:][i])\n",
    "    external_valid_tumor_select_feature.append(external_valid_tumor_feature.values[:,k+1])\n",
    "\n",
    "external_valid_tumor_select_feature = np.array(external_valid_tumor_select_feature).T\n",
    "\n",
    "external_valid_tumor_border_select_feature = []\n",
    "for i in range(len(border_select.columns[1:])):\n",
    "    k = int(border_select.columns[1:][i])\n",
    "    external_valid_tumor_border_select_feature.append(external_valid_border_feature.values[:,k+1])\n",
    "\n",
    "external_valid_tumor_border_select_feature = np.array(external_valid_tumor_border_select_feature).T\n",
    "\n",
    "external_DL_select_feature = []\n",
    "for i in range(len(DL_select.columns[2:])):\n",
    "    k = int(DL_select.columns[2:][i])\n",
    "    external_DL_select_feature.append(external_valid_DL_feature.values[:,k+2])\n",
    "\n",
    "external_DL_select_feature = np.array(external_DL_select_feature).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "external_valid_tumor_select_feature = standard_scaler.fit_transform(external_valid_tumor_select_feature)\n",
    "external_valid_tumor_select_feature = min_max_scaler.fit_transform(external_valid_tumor_select_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def Machine_learning(feature_inner, label_inner, feature_outer, label_outer,model):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    best_auc = 0.0\n",
    "    valid_max = []\n",
    "    test_max = []\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    test_auc_total = []\n",
    "    valid_auc_total = []\n",
    "\n",
    "    for train, test in skf.split(feature_inner, label_inner):\n",
    "        data_train, data_test = feature_inner[train], feature_inner[test]\n",
    "        label_train, label_test = label_inner[train], label_inner[test]\n",
    "\n",
    "        # 在训练集上训练模型\n",
    "\n",
    "        model.fit(data_train, label_train)\n",
    "        test_prob = model.predict_proba(data_test)[:, 1]  # 得到正类别的概率\n",
    "        test_auc = roc_auc_score(label_test, test_prob)\n",
    "\n",
    "        valid_prob = model.predict_proba(feature_outer)[:, 1]\n",
    "        valid_auc = roc_auc_score(label_outer, valid_prob)\n",
    "\n",
    "        test_auc_total.append(test_auc)\n",
    "        valid_auc_total.append(valid_auc)\n",
    "\n",
    "        # if test_auc >= valid_auc:\n",
    "        #     test_auc_total.append(test_auc)\n",
    "        #     valid_auc_total.append(valid_auc)\n",
    "\n",
    "    print(f\"{model.__class__.__name__} Test_AUC: \")\n",
    "    print(test_auc_total)\n",
    "    print(f\"{model.__class__.__name__} Valid_AUC: \")\n",
    "    print(valid_auc_total)\n",
    "    # k = 0.0\n",
    "    # for i in range(len(test_auc_total)):\n",
    "    #     if (test_auc_total[i]+valid_auc_total[i])/2 > k:\n",
    "    #         k = (test_auc_total[i]+valid_auc_total[i])/2\n",
    "    # return k\n",
    "    test_max = np.array(test_auc_total)\n",
    "    valid_max = np.array(valid_auc_total)\n",
    "\n",
    "    if test_max.max() >= valid_max.max():\n",
    "        return (valid_max.max()+test_max.max())/2 \n",
    "    else :\n",
    "        a = 0.0\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_select_auc(feature_inner, label_inner, feature_outer, label_outer,model):\n",
    "\n",
    "    best_auc = 0.0\n",
    "    index = []\n",
    "    #use_feature为当前记录下的特征\n",
    "    for j in range(5):\n",
    "        if j == 0:\n",
    "            for i in range(feature_inner.shape[1]):\n",
    "                model_feature = feature_inner[:,i].reshape(-1,1)\n",
    "                valid_feature = feature_outer[:,i].reshape(-1,1)\n",
    "                auc = Machine_learning(model_feature, label_inner, valid_feature, label_outer, model)\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    use_feature = model_feature\n",
    "                    use_valid_feature = valid_feature\n",
    "                    a = i\n",
    "\n",
    "            index.append(a)\n",
    "            index = np.array(index)\n",
    "\n",
    "        elif j == 1:\n",
    "            for i in range(feature_inner.shape[1]):\n",
    "                if i not in index:\n",
    "                    model_feature = feature_inner[:,i].reshape(-1,1)\n",
    "                    valid_feature = feature_outer[:,i].reshape(-1,1)\n",
    "                    if Machine_learning(np.concatenate((use_feature, model_feature),axis=1), label_inner, np.concatenate((use_valid_feature, valid_feature),axis=1), label_outer, model) >= best_auc:\n",
    "                    # if logistic_classic(np.concatenate((use_feature, model_feature),axis=1), label) >= best_auc:\n",
    "                        best_auc = Machine_learning(np.concatenate((use_feature, model_feature),axis=1), label_inner, np.concatenate((use_valid_feature, valid_feature),axis=1), label_outer, model)\n",
    "                        use_feature = np.concatenate((use_feature, model_feature),axis=1)\n",
    "                        use_valid_feature = np.concatenate((use_valid_feature, valid_feature),axis=1)\n",
    "                        index = np.concatenate((index,[i]))\n",
    "        elif j == 2:\n",
    "            for i in range(feature_inner.shape[1]):   \n",
    "                if i in index:\n",
    "                    if use_feature.shape[1] == 1:\n",
    "                        break\n",
    "                    mask = np.all(feature_inner[:,i] == use_feature.T, axis=1)\n",
    "                    if Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model) > best_auc:\n",
    "                        best_auc = Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model)\n",
    "                        use_feature = np.delete(use_feature, np.where(mask)[0], axis=1)\n",
    "                        use_valid_feature = np.delete(use_valid_feature, np.where(mask)[0], axis=1)\n",
    "                        index = np.delete(index, np.where(mask)[0],axis=0)\n",
    "                else:\n",
    "                    if Machine_learning(np.concatenate((use_feature, feature_inner[:,i].reshape(-1,1)),axis=1), label_inner, np.concatenate((use_valid_feature, feature_outer[:,i].reshape(-1,1)),axis=1),label_outer, model) >= best_auc:\n",
    "                        best_auc = Machine_learning(np.concatenate((use_feature, feature_inner[:,i].reshape(-1,1)),axis=1), label_inner, np.concatenate((use_valid_feature, feature_outer[:,i].reshape(-1,1)),axis=1),label_outer, model)\n",
    "                        use_feature = np.concatenate((use_feature, feature_inner[:,i].reshape(-1,1)),axis=1)\n",
    "                        use_valid_feature = np.concatenate((use_valid_feature, feature_outer[:,i].reshape(-1,1)),axis=1)\n",
    "                        index = np.concatenate((index,[i]))\n",
    "        elif j == 3:\n",
    "            for i in range(feature_inner.shape[1]):\n",
    "                if i in index:\n",
    "                    if use_feature.shape[1] == 1:\n",
    "                        break\n",
    "                    mask = np.all(feature_inner[:,i] == use_feature.T, axis=1)\n",
    "                    if Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model) > best_auc:\n",
    "                        Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model)\n",
    "                        use_feature = np.delete(use_feature, np.where(mask)[0], axis=1)\n",
    "                        use_valid_feature = np.delete(use_valid_feature, np.where(mask)[0], axis=1)\n",
    "                        index = np.delete(index, np.where(mask)[0],axis=0)\n",
    "        elif j == 4:\n",
    "            for i in range(feature_inner.shape[1]):\n",
    "                if i in index:\n",
    "                    if use_feature.shape[1] == 1:\n",
    "                        break\n",
    "                    mask = np.all(feature_inner[:,i] == use_feature.T, axis=1)\n",
    "                    if Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model) >= best_auc:\n",
    "                        best_auc = Machine_learning(np.delete(use_feature, np.where(mask)[0], axis=1), label_inner, np.delete(use_valid_feature, np.where(mask)[0], axis=1),label_outer, model)\n",
    "                        use_feature = np.delete(use_feature, np.where(mask)[0], axis=1)\n",
    "                        use_valid_feature = np.delete(use_valid_feature, np.where(mask)[0], axis=1)\n",
    "                        index = np.delete(index, np.where(mask)[0],axis=0)\n",
    "    \n",
    "    return best_auc, use_feature, use_valid_feature, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def Machine_Learning_alpha(feature_inner, label_inner, feature_outer, label_outer, model):\n",
    "\n",
    "    A = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "\n",
    "    for train, test in skf.split(feature_inner, label_inner):\n",
    "        data_train, data_test = feature_inner[train], feature_inner[test]\n",
    "        label_train, label_test = label_inner[train], label_inner[test]\n",
    "\n",
    "        # 在训练集上训练模型\n",
    "\n",
    "        model.fit(data_train, label_train)\n",
    "        A.append(bootstrap_ci(model, data_test, label_test))\n",
    "        A.append(bootstrap_ci(model, feature_outer, label_outer))\n",
    "\n",
    "    \n",
    "    return A\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "\n",
    "def bootstrap_ci(model, X_test, y_test, num_bootstrap=1000, alpha=0.95):\n",
    "    metrics = {\n",
    "        \"AUC\": [],\n",
    "        \"F1\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"Accuracy\": [],\n",
    "        \"Average Precision (AP)\": [],\n",
    "    }\n",
    "    n_size = len(y_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # y_pred = model.predict(X_test)\n",
    "    threshold = calculate_threshold(y_test, y_pred_proba, expected_metric=\"Youden\", expected_value=0.9)\n",
    "    \n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    for _ in range(num_bootstrap):\n",
    "        indices = np.random.choice(range(n_size), size=n_size, replace=True)\n",
    "        if len(np.unique(y_test[indices])) < 2:\n",
    "            continue\n",
    "        cm = confusion_matrix(y_test[indices], y_pred[indices])\n",
    "\n",
    "        \n",
    "        auc = roc_auc_score(y_test[indices], y_pred_proba[indices])\n",
    "        f1 = f1_score(y_test[indices], y_pred[indices])\n",
    "        precision = precision_score(y_test[indices], y_pred[indices])\n",
    "        recall = recall_score(y_test[indices], y_pred[indices])\n",
    "        accuracy = accuracy_score(y_test[indices], y_pred[indices])\n",
    "        ap = average_precision_score(y_test[indices], y_pred_proba[indices])\n",
    "        \n",
    "        metrics[\"AUC\"].append(auc)\n",
    "        metrics[\"F1\"].append(f1)\n",
    "        metrics[\"Precision\"].append(precision)\n",
    "        metrics[\"Recall\"].append(recall)\n",
    "        metrics[\"Accuracy\"].append(accuracy)\n",
    "        metrics[\"Average Precision (AP)\"].append(ap)\n",
    "\n",
    "    ci_results = {}\n",
    "    for metric, scores in metrics.items():\n",
    "        lower = np.percentile(scores, ((1.0 - alpha) / 2.0) * 100)\n",
    "        upper = np.percentile(scores, (alpha + ((1.0 - alpha) / 2.0)) * 100)\n",
    "        ci_results[metric] = (np.mean(scores), (lower, upper))\n",
    "    \n",
    "    return ci_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(y_true, y_score, expected_metric=\"Youden\", expected_value=0.5):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "\n",
    "    if expected_metric == \"Youden\":\n",
    "        j_scores = tpr - fpr\n",
    "        j_ordered = sorted(zip(j_scores, thresholds), reverse=True)\n",
    "        threshold = j_ordered[0][1]\n",
    "    elif expected_metric.lower() == \"sen\" or expected_metric.lower() == \"sensitivity\":\n",
    "        idx = np.where(tpr >= expected_value)[0]\n",
    "        threshold = thresholds[idx[0]]\n",
    "    else:\n",
    "        raise ValueError(\"Not supported expected metric: {}\".format(expected_metric))\n",
    "\n",
    "    return threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "def plot_curves(models, X_data, X_label, Y_data, Y_label, X_ci_results, Y_ci_results, X_kflod_index, Y_kflod_index, judge):\n",
    "    colors = plt.cm.get_cmap('tab10', len(models))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if judge == True:\n",
    "        p = 'Test'\n",
    "        ci_results = X_ci_results\n",
    "        kflod_index = X_kflod_index\n",
    "    else:\n",
    "        p = 'Valid'\n",
    "        ci_results = Y_ci_results\n",
    "        kflod_index = Y_kflod_index\n",
    "\n",
    "    plt.figure(figsize=(20, 40))\n",
    "\n",
    "    # ROC曲线\n",
    "    plt.subplot(5, 2, 1)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model_X_data = X_data[i]  \n",
    "        model_Y_data = Y_data[i]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        fpr_train, tpr_train, _ = roc_curve(label_test, y_pred_proba_train)\n",
    "        plt.plot(fpr_train, tpr_train, label=f'{name} {p} (AUC: {ci_results[name][\"AUC\"][0]:.3f} [{ci_results[name][\"AUC\"][1][0]:.3f}, {ci_results[name][\"AUC\"][1][1]:.3f}])', color=colors(i))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 'k'代表黑色，'--'代表虚线\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    # PR曲线\n",
    "    plt.subplot(5, 2, 3)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model_X_data = X_data[i]  \n",
    "        model_Y_data = Y_data[i]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        precision_train, recall_train, _ = precision_recall_curve(label_test, y_pred_proba_train)\n",
    "        plt.plot(recall_train, precision_train, label=f'{name} {p} (AP: {ci_results[name][\"Average Precision (AP)\"][0]:.3f} [{ci_results[name][\"Average Precision (AP)\"][1][0]:.3f}, {ci_results[name][\"Average Precision (AP)\"][1][1]:.3f}])', color=colors(i))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    #图表位置\n",
    "    # plt.legend(loc='lower right')\n",
    "\n",
    "    # 校准曲线\n",
    "    plt.subplot(5, 2, 5)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model_X_data = X_data[i]  \n",
    "        model_Y_data = Y_data[i]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        prob_true_train, prob_pred_train = calibration_curve(label_test, y_pred_proba_train, n_bins=10)\n",
    "        plt.plot(prob_pred_train, prob_true_train, label=f'{name} {p}', color=colors(i))\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'Calibration Curve - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    # 决策曲线分析 (DCA)\n",
    "    plt.subplot(5, 2, 7)\n",
    "    thresholds = np.linspace(0.01, 0.99, 10)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model_X_data = X_data[i]  \n",
    "        model_Y_data = Y_data[i]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        net_benefits_train = [net_benefit(y_pred_proba_train, label_test, threshold) for threshold in thresholds]\n",
    "        plt.plot(thresholds, net_benefits_train, label=f'{name} {p}', color=colors(i))\n",
    "    treat_all_train = [np.sum(label_test) / len(label_test) - threshold / (1 - threshold) for threshold in thresholds]\n",
    "    treat_none_train = [0 for _ in thresholds]\n",
    "    plt.plot(thresholds, treat_all_train, linestyle='--', color='gray', label=f'Treat All {p}')\n",
    "    plt.plot(thresholds, treat_none_train, linestyle='--', color='black', label=f'Treat None {p}')\n",
    "    plt.ylim([-1.0, 0.4])\n",
    "    plt.xlabel('Threshold Probability')\n",
    "    plt.ylabel('Net Benefit')\n",
    "    plt.title(f'Decision Curve Analysis - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    # 模型预测概率分布\n",
    "    plt.subplot(5, 2, 9)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model_X_data = X_data[i]  \n",
    "        model_Y_data = Y_data[i]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "        plt.hist(y_pred_proba_train, bins=20, alpha=0.5, label=f'{name} {p}', color=colors(i))\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Predicted Probability Distribution - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "def net_benefit(probabilities, y_true, threshold):\n",
    "        tp = np.sum((probabilities >= threshold) & (y_true == 1))\n",
    "        fp = np.sum((probabilities >= threshold) & (y_true == 0))\n",
    "        fn = np.sum((probabilities < threshold) & (y_true == 1))\n",
    "        tn = np.sum((probabilities < threshold) & (y_true == 0))\n",
    "        \n",
    "        prevalence = (tp + fn) / (tp + fp + tn + fn)\n",
    "        net_benefit = tp / (tp + fp + tn + fn) - (fp / (tp + fp + tn + fn)) * (threshold / (1 - threshold))\n",
    "        return net_benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "def one_model_plot_curves(models, X_data, X_label, Y_data, Y_label, X_ci_results, Y_ci_results, X_kflod_index, Y_kflod_index, judge, select_files, valid_select_files):\n",
    "    colors = plt.cm.get_cmap('tab10', len(X_data))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if judge == True:\n",
    "        p = 'Test'\n",
    "        ci_results = X_ci_results\n",
    "        kflod_index = X_kflod_index\n",
    "        name = select_files\n",
    "    else:\n",
    "        p = 'Valid'\n",
    "        ci_results = Y_ci_results\n",
    "        kflod_index = Y_kflod_index\n",
    "        name = valid_select_files\n",
    "\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    \n",
    "    #混淆矩阵\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        models.fit(data_train, label_train)\n",
    "        y_pred_proba_train = models.predict_proba(data_test)[:, 1]\n",
    "        y_pred = models.predict(data_test)\n",
    "    \n",
    "        # 计算混淆矩阵\n",
    "        cm = confusion_matrix(label_test, y_pred)\n",
    "        \n",
    "        # 输出混淆矩阵\n",
    "        print(f\"Confusion Matrix for fold {i+1}:\")\n",
    "        print(cm)\n",
    "\n",
    "\n",
    "    # ROC曲线\n",
    "    plt.subplot(5, 2, 1)\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        models.fit(data_train, label_train)\n",
    "        y_pred_proba_train = models.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        fpr_train, tpr_train, _ = roc_curve(label_test, y_pred_proba_train)\n",
    "        plt.plot(fpr_train, tpr_train, label=f'{name[i]} {p} (AUC: {ci_results[name[i]][\"AUC\"][0]:.3f} [{ci_results[name[i]][\"AUC\"][1][0]:.3f}, {ci_results[name[i]][\"AUC\"][1][1]:.3f}])', color=colors(i))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 'k'代表黑色，'--'代表虚线\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {p}')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.svg', format='svg', bbox_inches='tight')\n",
    "    # PR曲线\n",
    "    plt.subplot(5, 2, 3)\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        precision_train, recall_train, _ = precision_recall_curve(label_test, y_pred_proba_train)\n",
    "        plt.plot(recall_train, precision_train, label=f'{name[i]} {p} (AP: {ci_results[name[i]][\"Average Precision (AP)\"][0]:.3f} [{ci_results[name[i]][\"Average Precision (AP)\"][1][0]:.3f}, {ci_results[name[i]][\"Average Precision (AP)\"][1][1]:.3f}])', color=colors(i))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    #图表位置\n",
    "    # plt.legend(loc='lower right')\n",
    "\n",
    "    # 校准曲线\n",
    "    plt.subplot(5, 2, 5)\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        prob_true_train, prob_pred_train = calibration_curve(label_test, y_pred_proba_train, n_bins=10)\n",
    "        plt.plot(prob_pred_train, prob_true_train, label=f'{name[i]} {p}', color=colors(i))\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'Calibration Curve - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    # 决策曲线分析 (DCA)\n",
    "    plt.subplot(5, 2, 7)\n",
    "    thresholds = np.linspace(0.01, 0.99, 10)\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "\n",
    "        net_benefits_train = [net_benefit(y_pred_proba_train, label_test, threshold) for threshold in thresholds]\n",
    "        plt.plot(thresholds, net_benefits_train, label=f'{name[i]} {p}', color=colors(i))\n",
    "    treat_all_train = [np.sum(label_test) / len(label_test) - threshold / (1 - threshold) for threshold in thresholds]\n",
    "    treat_none_train = [0 for _ in thresholds]\n",
    "    plt.plot(thresholds, treat_all_train, linestyle='--', color='gray', label=f'Treat All {p}')\n",
    "    plt.plot(thresholds, treat_none_train, linestyle='--', color='black', label=f'Treat None {p}')\n",
    "    plt.ylim([-1.0, 0.4])\n",
    "    plt.xlabel('Threshold Probability')\n",
    "    plt.ylabel('Net Benefit')\n",
    "    plt.title(f'Decision Curve Analysis - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "    # 模型预测概率分布\n",
    "    plt.subplot(5, 2, 9)\n",
    "    for i in range(len(X_data)):\n",
    "        model_X_data = X_data[f'select_{i}'][0]\n",
    "        model_Y_data = Y_data[f'valid_select_{i}'][0]\n",
    "        j=0\n",
    "        for train, test in skf.split(model_X_data, X_label):\n",
    "            data_train, data_test = model_X_data[train], model_X_data[test]\n",
    "            label_train, label_test = X_label[train], X_label[test]\n",
    "            if j == kflod_index[i]:\n",
    "                break\n",
    "            j=j+1\n",
    "        if judge == False:\n",
    "            data_test, label_test = model_Y_data, Y_label\n",
    "            \n",
    "        #训练数据选择\n",
    "        model.fit(data_train, label_train)\n",
    "        y_pred_proba_train = model.predict_proba(data_test)[:, 1]\n",
    "        plt.hist(y_pred_proba_train, bins=20, alpha=0.5, label=f'{name[i]} {p}', color=colors(i))\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Predicted Probability Distribution - {p}')\n",
    "    plt.legend()\n",
    "\n",
    "def net_benefit(probabilities, y_true, threshold):\n",
    "        tp = np.sum((probabilities >= threshold) & (y_true == 1))\n",
    "        fp = np.sum((probabilities >= threshold) & (y_true == 0))\n",
    "        fn = np.sum((probabilities < threshold) & (y_true == 1))\n",
    "        tn = np.sum((probabilities < threshold) & (y_true == 0))\n",
    "        \n",
    "        prevalence = (tp + fn) / (tp + fp + tn + fn)\n",
    "        net_benefit = tp / (tp + fp + tn + fn) - (fp / (tp + fp + tn + fn)) * (threshold / (1 - threshold))\n",
    "        return net_benefit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
