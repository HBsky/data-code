{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 加载示例数据集\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 定义特征提取算法\n",
    "feature_extractors = {\n",
    "    \"PCA\": PCA(n_components=2),\n",
    "    \"SelectKBest\": SelectKBest(f_classif, k=2),\n",
    "    \"LassoCV\": LassoCV(),\n",
    "    \"RFE\": RFE(estimator=LogisticRegression(), n_features_to_select=2),\n",
    "    # 其他特征提取算法可以在此添加\n",
    "}\n",
    "\n",
    "# 提取特征并绘制图表\n",
    "for name, extractor in feature_extractors.items():\n",
    "    if name in [\"LassoCV\", \"RFE\"]:\n",
    "        extractor.fit(X_scaled, y)\n",
    "        if name == \"LassoCV\":\n",
    "            feature_weights = extractor.coef_\n",
    "        else:\n",
    "            feature_weights = extractor.ranking_\n",
    "    else:\n",
    "        X_new = extractor.fit_transform(X_scaled, y)\n",
    "        feature_weights = extractor.explained_variance_ratio_ if name == \"PCA\" else extractor.scores_\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(feature_weights)), feature_weights)\n",
    "    plt.title(f\"Feature Weights using {name}\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "#########################模型优化#################################\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 定义模型和调优空间\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression,\n",
    "    \"RandomForest\": RandomForestClassifier,\n",
    "    \"SVM\": SVC,\n",
    "    \"XGBoost\": XGBClassifier,\n",
    "    \"LightGBM\": LGBMClassifier,\n",
    "    # 其他模型可以在此添加\n",
    "}\n",
    "\n",
    "def objective(trial, model_class):\n",
    "    if model_class == LogisticRegression:\n",
    "        params = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-5, 1e2),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "        }\n",
    "    elif model_class == RandomForestClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        }\n",
    "    elif model_class == SVC:\n",
    "        params = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-5, 1e2),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "            \"probability\":trial.suggest_categorical(\"probability\", [True])\n",
    "        }\n",
    "    elif model_class == XGBClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1.0)\n",
    "        }\n",
    "    elif model_class == LGBMClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1.0)\n",
    "        }\n",
    "\n",
    "    model = model_class(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=make_scorer(roc_auc_score)).mean()\n",
    "    return score\n",
    "\n",
    "optimized_models = {}\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_class), n_trials=50)\n",
    "    optimized_models[name] = model_class(**study.best_params)\n",
    "\n",
    "    print(f\"Best parameters for {name}: {study.best_params}\")\n",
    "\n",
    "\n",
    "\n",
    "########################模型评估#############################\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "\n",
    "def bootstrap_ci(model, X_test, y_test, num_bootstrap=1000, alpha=0.95):\n",
    "    metrics = {\n",
    "        \"AUC\": [],\n",
    "        \"F1\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"Accuracy\": [],\n",
    "        \"Average Precision (AP)\": []\n",
    "    }\n",
    "    n_size = len(y_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    for _ in range(num_bootstrap):\n",
    "        indices = np.random.choice(range(n_size), size=n_size, replace=True)\n",
    "        if len(np.unique(y_test[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        auc = roc_auc_score(y_test[indices], y_pred_proba[indices])\n",
    "        f1 = f1_score(y_test[indices], y_pred[indices])\n",
    "        precision = precision_score(y_test[indices], y_pred[indices])\n",
    "        recall = recall_score(y_test[indices], y_pred[indices])\n",
    "        accuracy = accuracy_score(y_test[indices], y_pred[indices])\n",
    "        ap = average_precision_score(y_test[indices], y_pred_proba[indices])\n",
    "        \n",
    "        metrics[\"AUC\"].append(auc)\n",
    "        metrics[\"F1\"].append(f1)\n",
    "        metrics[\"Precision\"].append(precision)\n",
    "        metrics[\"Recall\"].append(recall)\n",
    "        metrics[\"Accuracy\"].append(accuracy)\n",
    "        metrics[\"Average Precision (AP)\"].append(ap)\n",
    "    \n",
    "    ci_results = {}\n",
    "    for metric, scores in metrics.items():\n",
    "        lower = np.percentile(scores, ((1.0 - alpha) / 2.0) * 100)\n",
    "        upper = np.percentile(scores, (alpha + ((1.0 - alpha) / 2.0)) * 100)\n",
    "        ci_results[metric] = (np.mean(scores), (lower, upper))\n",
    "    \n",
    "    return ci_results\n",
    "\n",
    "# 计算评估指标的置信区间\n",
    "'''for name, model in optimized_models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        results_table = bootstrap_ci(model, X_test, y_test)\n",
    "        print(f\"{name} Classification CI Results:\")\n",
    "        print(results_table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model {name}: {e}\")'''\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for name, model in optimized_models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        results_table = bootstrap_ci(model, X_test, y_test)\n",
    "        results_dict[name] = results_table\n",
    "        print(f\"{name} Classification CI Results:\")\n",
    "        print(results_table)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model {name}: {e}\")\n",
    "\n",
    "print(results_dict)\n",
    "\n",
    "\n",
    "##############################模型绘图###################################\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "def plot_curves(models, X_train, y_train, X_test, y_test, ci_results):\n",
    "    colors = plt.cm.get_cmap('tab10', len(models))\n",
    "    \n",
    "    plt.figure(figsize=(20, 40))\n",
    "    \n",
    "    # ROC曲线 - 训练集\n",
    "    plt.subplot(5, 2, 1)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_proba_train)\n",
    "        plt.plot(fpr_train, tpr_train, label=f'{name} Train (AUC: {ci_results[name][\"AUC\"][0]:.2f} [{ci_results[name][\"AUC\"][1][0]:.2f}, {ci_results[name][\"AUC\"][1][1]:.2f}])', color=colors(i))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Train')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC曲线 - 测试集\n",
    "    plt.subplot(5, 2, 2)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba_test)\n",
    "        plt.plot(fpr_test, tpr_test, label=f'{name} Test (AUC: {ci_results[name][\"AUC\"][0]:.2f} [{ci_results[name][\"AUC\"][1][0]:.2f}, {ci_results[name][\"AUC\"][1][1]:.2f}])', color=colors(i))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Test')\n",
    "    plt.legend()\n",
    "    \n",
    "    # PR曲线 - 训练集\n",
    "    plt.subplot(5, 2, 3)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        precision_train, recall_train, _ = precision_recall_curve(y_train, y_pred_proba_train)\n",
    "        plt.plot(recall_train, precision_train, label=f'{name} Train (AP: {ci_results[name][\"Average Precision (AP)\"][0]:.2f} [{ci_results[name][\"Average Precision (AP)\"][1][0]:.2f}, {ci_results[name][\"Average Precision (AP)\"][1][1]:.2f}])', color=colors(i))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve - Train')\n",
    "    plt.legend()\n",
    "    \n",
    "    # PR曲线 - 测试集\n",
    "    plt.subplot(5, 2, 4)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        precision_test, recall_test, _ = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "        plt.plot(recall_test, precision_test, label=f'{name} Test (AP: {ci_results[name][\"Average Precision (AP)\"][0]:.2f} [{ci_results[name][\"Average Precision (AP)\"][1][0]:.2f}, {ci_results[name][\"Average Precision (AP)\"][1][1]:.2f}])', color=colors(i))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve - Test')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 校准曲线 - 训练集hui\n",
    "    plt.subplot(5, 2, 5)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        prob_true_train, prob_pred_train = calibration_curve(y_train, y_pred_proba_train, n_bins=10)\n",
    "        plt.plot(prob_pred_train, prob_true_train, label=f'{name} Train', color=colors(i))\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Curve - Train')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 校准曲线 - 测试集\n",
    "    plt.subplot(5, 2, 6)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        prob_true_test, prob_pred_test = calibration_curve(y_test, y_pred_proba_test, n_bins=10)\n",
    "        plt.plot(prob_pred_test, prob_true_test, label=f'{name} Test', color=colors(i))\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Curve - Test')\n",
    "    plt.legend()\n",
    "\n",
    "    # 决策曲线分析 (DCA) - 训练集\n",
    "    plt.subplot(5, 2, 7)\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        net_benefits_train = [net_benefit(y_pred_proba_train, y_train, threshold) for threshold in thresholds]\n",
    "        plt.plot(thresholds, net_benefits_train, label=f'{name} Train', color=colors(i))\n",
    "    treat_all_train = [np.sum(y_train) / len(y_train) - threshold / (1 - threshold) for threshold in thresholds]\n",
    "    treat_none_train = [0 for _ in thresholds]\n",
    "    plt.plot(thresholds, treat_all_train, linestyle='--', color='gray', label='Treat All Train')\n",
    "    plt.plot(thresholds, treat_none_train, linestyle='--', color='black', label='Treat None Train')\n",
    "    plt.xlabel('Threshold Probability')\n",
    "    plt.ylabel('Net Benefit')\n",
    "    plt.title('Decision Curve Analysis - Train')\n",
    "    plt.legend()\n",
    "\n",
    "    # 决策曲线分析 (DCA) - 测试集\n",
    "    plt.subplot(5, 2, 8)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        net_benefits_test = [net_benefit(y_pred_proba_test, y_test, threshold) for threshold in thresholds]\n",
    "        plt.plot(thresholds, net_benefits_test, label=f'{name} Test', color=colors(i))\n",
    "    treat_all_test = [np.sum(y_test) / len(y_test) - threshold / (1 - threshold) for threshold in thresholds]\n",
    "    treat_none_test = [0 for _ in thresholds]\n",
    "    plt.plot(thresholds, treat_all_test, linestyle='--', color='gray', label='Treat All Test')\n",
    "    plt.plot(thresholds, treat_none_test, linestyle='--', color='black', label='Treat None Test')\n",
    "    plt.xlabel('Threshold Probability')\n",
    "    plt.ylabel('Net Benefit')\n",
    "    plt.title('Decision Curve Analysis - Test')\n",
    "    plt.legend()\n",
    "\n",
    "    # 模型预测概率分布 - 训练集\n",
    "    plt.subplot(5, 2, 9)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        plt.hist(y_pred_proba_train, bins=20, alpha=0.5, label=f'{name} Train', color=colors(i))\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Predicted Probability Distribution - Train')\n",
    "    plt.legend()\n",
    "\n",
    "    # 模型预测概率分布 - 测试集\n",
    "    plt.subplot(5, 2, 10)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        plt.hist(y_pred_proba_test, bins=20, alpha=0.5, label=f'{name} Test', color=colors(i))\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Predicted Probability Distribution - Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def net_benefit(probabilities, y_true, threshold):\n",
    "        tp = np.sum((probabilities >= threshold) & (y_true == 1))\n",
    "        fp = np.sum((probabilities >= threshold) & (y_true == 0))\n",
    "        fn = np.sum((probabilities < threshold) & (y_true == 1))\n",
    "        tn = np.sum((probabilities < threshold) & (y_true == 0))\n",
    "        \n",
    "        prevalence = (tp + fn) / (tp + fp + tn + fn)\n",
    "        net_benefit = tp / (tp + fp + tn + fn) - (fp / (tp + fp + tn + fn)) * (threshold / (1 - threshold))\n",
    "        return net_benefit\n",
    "\n",
    "\n",
    "plot_curves(models=optimized_models,X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,ci_results=results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(y_true, y_score, expected_metric=\"Youden\", expected_value=0.5):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "\n",
    "    if expected_metric == \"Youden\":\n",
    "        j_scores = tpr - fpr\n",
    "        j_ordered = sorted(zip(j_scores, thresholds), reverse=True)\n",
    "        threshold = j_ordered[0][1]\n",
    "    elif expected_metric.lower() == \"sen\" or expected_metric.lower() == \"sensitivity\":\n",
    "        idx = np.where(tpr >= expected_value)[0]\n",
    "        threshold = thresholds[idx[0]]\n",
    "    else:\n",
    "        raise ValueError(\"Not supported expected metric: {}\".format(expected_metric))\n",
    "\n",
    "    return threshold\n",
    "\n",
    "calculate_threshold(y_true, y_score, expected_metric=\"sensitivity\", expected_value=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
